# Breadcrumb Research Agent Evaluation Config
# Evals the REAL bc-researcher agent (agents/bc-researcher.md)
#
# Run: pnpm eval
# View: pnpm eval:view

description: "bc-researcher agent quality evaluation"

providers:
  - id: file://provider.ts
    config:
      model: claude-sonnet-4-5-20250929

prompts:
  - id: research-topic
    raw: |
      Research the following topic and produce a structured research document.

      Topic: {{topic}}

defaultTest:
  assert:
    # Structural completeness - every research doc needs these sections
    - type: contains
      value: "Recommended Stack"
    - type: contains
      value: "Pitfalls"
    - type: javascript
      value: "output.toLowerCase().includes('sources') || output.toLowerCase().includes('references')"

tests:
  # Topic 1: Library Selection (React Form Library)
  - vars:
      topic: "Best form validation library for a React TypeScript project in 2026. Needs async validation, nested objects, custom error messages, and TypeScript inference."
    assert:
      - type: llm-rubric
        value: |
          Evaluate this technical research document across these dimensions:

          1. ACCURACY (0.0-0.25): Are library versions current? Are API examples correct?
             0.25 = all claims verified and current
             0.15 = mostly verified, minor gaps
             0.05 = outdated or unverified claims

          2. ACTIONABILITY (0.0-0.25): Does it prescribe a clear choice?
             0.25 = clear "use X because Y" with justification
             0.15 = recommendation present but hedged
             0.05 = exploratory "consider X or Y"

          3. SOURCE QUALITY (0.0-0.25): Are sources authoritative with confidence levels?
             0.25 = official docs cited with HIGH/MEDIUM/LOW confidence
             0.15 = some sources, confidence unclear
             0.05 = no sources or low-quality

          4. PITFALLS (0.0-0.25): Are specific pitfalls documented?
             0.25 = 2+ specific pitfalls with prevention strategies
             0.15 = 1-2 somewhat specific pitfalls
             0.05 = generic or no pitfalls

          Output a score from 0.0 to 1.0.
        threshold: 0.7
      - type: llm-rubric
        value: |
          Compare this research document against the golden reference below.
          Score how well it covers the same key topics, libraries, patterns, and pitfalls.
          1.0 = covers all the same ground with equal or better depth
          0.5 = covers most topics but misses some key points
          0.0 = largely different content or missing major sections

          Golden reference:
          file://golden-references/react-form-library.txt
        threshold: 0.7
      - type: contains
        value: "Don't Hand-Roll"

  # Topic 2: Library Selection (Charting Library)
  - vars:
      topic: "Best React charting library for a dashboard application. Stack uses Tailwind CSS and Radix UI. Needs line charts, bar charts, donut charts, and dark theme support."
    assert:
      - type: llm-rubric
        value: |
          Evaluate this technical research document across these dimensions:

          1. ACCURACY (0.0-0.25): Are library versions current? Are API examples correct?
             0.25 = all claims verified and current
             0.15 = mostly verified, minor gaps
             0.05 = outdated or unverified claims

          2. ACTIONABILITY (0.0-0.25): Does it prescribe a clear choice?
             0.25 = clear "use X because Y" with justification
             0.15 = recommendation present but hedged
             0.05 = exploratory "consider X or Y"

          3. SOURCE QUALITY (0.0-0.25): Are sources authoritative with confidence levels?
             0.25 = official docs cited with HIGH/MEDIUM/LOW confidence
             0.15 = some sources, confidence unclear
             0.05 = no sources or low-quality

          4. PITFALLS (0.0-0.25): Are specific pitfalls documented?
             0.25 = 2+ specific pitfalls with prevention strategies
             0.15 = 1-2 somewhat specific pitfalls
             0.05 = generic or no pitfalls

          Output a score from 0.0 to 1.0.
        threshold: 0.7
      - type: llm-rubric
        value: |
          Compare this research document against the golden reference below.
          Score how well it covers the same key topics, libraries, patterns, and pitfalls.
          1.0 = covers all the same ground with equal or better depth
          0.5 = covers most topics but misses some key points
          0.0 = largely different content or missing major sections

          Golden reference:
          file://golden-references/charting-library.txt
        threshold: 0.7
      - type: contains
        value: "Don't Hand-Roll"

  # Topic 3: API Integration (Stripe)
  - vars:
      topic: "How to integrate Stripe payment processing into an Express.js application. Must handle checkout sessions, webhook verification, subscription management, and failed payments."
    assert:
      - type: llm-rubric
        value: |
          Evaluate this technical research document across these dimensions:

          1. ACCURACY (0.0-0.25): Are library versions current? Are API examples correct?
             0.25 = all claims verified and current
             0.15 = mostly verified, minor gaps
             0.05 = outdated or unverified claims

          2. ACTIONABILITY (0.0-0.25): Does it prescribe a clear choice?
             0.25 = clear "use X because Y" with justification
             0.15 = recommendation present but hedged
             0.05 = exploratory "consider X or Y"

          3. SOURCE QUALITY (0.0-0.25): Are sources authoritative with confidence levels?
             0.25 = official docs cited with HIGH/MEDIUM/LOW confidence
             0.15 = some sources, confidence unclear
             0.05 = no sources or low-quality

          4. PITFALLS (0.0-0.25): Are specific pitfalls documented?
             0.25 = 2+ specific pitfalls with prevention strategies
             0.15 = 1-2 somewhat specific pitfalls
             0.05 = generic or no pitfalls

          Output a score from 0.0 to 1.0.
        threshold: 0.7
      - type: llm-rubric
        value: |
          Compare this research document against the golden reference below.
          Score how well it covers the same key topics, libraries, patterns, and pitfalls.
          1.0 = covers all the same ground with equal or better depth
          0.5 = covers most topics but misses some key points
          0.0 = largely different content or missing major sections

          Golden reference:
          file://golden-references/stripe-integration.txt
        threshold: 0.7
      - type: contains
        value: "webhook"
      - type: contains
        value: "Don't Hand-Roll"

  # Topic 4: Tooling Comparison (Vite vs Webpack)
  - vars:
      topic: "Whether to use Vite or Webpack for a production React application in 2026. App has TypeScript, Tailwind CSS, needs code-splitting and lazy loading."
    assert:
      - type: llm-rubric
        value: |
          Evaluate this technical research document across these dimensions:

          1. ACCURACY (0.0-0.25): Are library versions current? Are API examples correct?
             0.25 = all claims verified and current
             0.15 = mostly verified, minor gaps
             0.05 = outdated or unverified claims

          2. ACTIONABILITY (0.0-0.25): Does it prescribe a clear choice?
             0.25 = clear "use X because Y" with justification
             0.15 = recommendation present but hedged
             0.05 = exploratory "consider X or Y"

          3. SOURCE QUALITY (0.0-0.25): Are sources authoritative with confidence levels?
             0.25 = official docs cited with HIGH/MEDIUM/LOW confidence
             0.15 = some sources, confidence unclear
             0.05 = no sources or low-quality

          4. PITFALLS (0.0-0.25): Are specific pitfalls documented?
             0.25 = 2+ specific pitfalls with prevention strategies
             0.15 = 1-2 somewhat specific pitfalls
             0.05 = generic or no pitfalls

          Output a score from 0.0 to 1.0.
        threshold: 0.7
      - type: llm-rubric
        value: |
          Compare this research document against the golden reference below.
          Score how well it covers the same key topics, libraries, patterns, and pitfalls.
          1.0 = covers all the same ground with equal or better depth
          0.5 = covers most topics but misses some key points
          0.0 = largely different content or missing major sections

          Golden reference:
          file://golden-references/vite-vs-webpack.txt
        threshold: 0.7
      - type: llm-rubric
        value: |
          Does the research make a clear prescriptive choice between the tools
          (not "both are valid")? Score 1.0 if clear choice, 0.0 if no choice.
        threshold: 0.8

  # Topic 5: Migration Guide (Express to Fastify)
  - vars:
      topic: "How to migrate an existing Express.js REST API to Fastify. The API has 50+ routes, middleware for auth and logging, MongoDB with Mongoose, and WebSocket support."
    assert:
      - type: llm-rubric
        value: |
          Evaluate this technical research document across these dimensions:

          1. ACCURACY (0.0-0.25): Are library versions current? Are API examples correct?
             0.25 = all claims verified and current
             0.15 = mostly verified, minor gaps
             0.05 = outdated or unverified claims

          2. ACTIONABILITY (0.0-0.25): Does it prescribe a clear choice?
             0.25 = clear "use X because Y" with justification
             0.15 = recommendation present but hedged
             0.05 = exploratory "consider X or Y"

          3. SOURCE QUALITY (0.0-0.25): Are sources authoritative with confidence levels?
             0.25 = official docs cited with HIGH/MEDIUM/LOW confidence
             0.15 = some sources, confidence unclear
             0.05 = no sources or low-quality

          4. PITFALLS (0.0-0.25): Are specific pitfalls documented?
             0.25 = 2+ specific pitfalls with prevention strategies
             0.15 = 1-2 somewhat specific pitfalls
             0.05 = generic or no pitfalls

          Output a score from 0.0 to 1.0.
        threshold: 0.7
      - type: llm-rubric
        value: |
          Compare this research document against the golden reference below.
          Score how well it covers the same key topics, libraries, patterns, and pitfalls.
          1.0 = covers all the same ground with equal or better depth
          0.5 = covers most topics but misses some key points
          0.0 = largely different content or missing major sections

          Golden reference:
          file://golden-references/express-to-fastify.txt
        threshold: 0.7
      - type: icontains
        value: "migration"
      - type: contains
        value: "Don't Hand-Roll"
