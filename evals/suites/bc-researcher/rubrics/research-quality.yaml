# Research Quality Rubric for bc-researcher Agent
#
# Scoring: 4 dimensions, 0.25 each, total 0.0-1.0
# Passing threshold: 0.7
# Judge model: Claude Sonnet, temperature: 0

rubric: |
  Evaluate this technical research document across these dimensions:

  1. ACCURACY (0.0-0.25): Are library versions current? Are API examples correct?
     0.25 = all claims verified and current (2025-2026 sources)
     0.20 = most claims verified, minor version/API gaps
     0.15 = some outdated info or unverified claims
     0.10 = multiple inaccuracies
     0.05 = largely outdated or incorrect
     0.00 = major inaccuracies throughout

  2. ACTIONABILITY (0.0-0.25): Does it prescribe a clear choice?
     0.25 = clear "use X because Y" with specific justification
     0.20 = strong recommendation, minor hedging
     0.15 = recommendation present but noticeably hedged
     0.10 = multiple options presented with weak preference
     0.05 = exploratory "consider X or Y" without decision
     0.00 = purely exploratory with no recommendation

  3. SOURCE QUALITY (0.0-0.25): Are sources authoritative with confidence levels?
     0.25 = official docs/Context7 cited with HIGH/MEDIUM/LOW confidence labels
     0.20 = good sources cited, confidence levels present but inconsistent
     0.15 = some sources cited, confidence levels unclear or missing
     0.10 = few sources, no confidence attribution
     0.05 = vague references only
     0.00 = no sources cited

  4. PITFALLS (0.0-0.25): Are specific pitfalls documented with prevention?
     0.25 = 3+ specific pitfalls with "What happens" and "Avoid by"
     0.20 = 2+ specific pitfalls with clear prevention
     0.15 = 1-2 pitfalls, somewhat specific
     0.10 = generic pitfalls (e.g., "test thoroughly")
     0.05 = pitfalls mentioned but no prevention strategies
     0.00 = no pitfalls section

  Output a single score from 0.0 (poor) to 1.0 (excellent).
  Briefly justify your score for each dimension.

threshold: 0.7

# Structural requirements (deterministic checks)
required_sections:
  - "Recommended Stack"
  - "Pitfalls"
  - "Sources"
  - "Don't Hand-Roll"
  - "Key Patterns"

# Prescriptiveness check
prescriptive_rubric: |
  Does the TL;DR section make a clear prescriptive recommendation
  ("Use X because Y") rather than exploratory options ("Consider X or Y")?
  Score 1.0 if prescriptive, 0.5 if somewhat prescriptive, 0.0 if exploratory.

prescriptive_threshold: 0.8
